{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import tensor \n",
    "import numpy as np\n",
    "\n",
    "torch.set_warn_always(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_backward_cls',\n",
       " '_compiled_autograd_backward_state',\n",
       " '_compiled_autograd_key',\n",
       " '_get_compiled_autograd_symints',\n",
       " '_is_compiled_autograd_tracing',\n",
       " '_materialize_non_diff_grads',\n",
       " '_raw_saved_tensors',\n",
       " '_register_hook',\n",
       " '_register_hook_dict',\n",
       " '_sequence_nr',\n",
       " '_set_sequence_nr',\n",
       " 'apply',\n",
       " 'backward',\n",
       " 'dirty_tensors',\n",
       " 'forward',\n",
       " 'generate_vmap_rule',\n",
       " 'is_traceable',\n",
       " 'jvp',\n",
       " 'mark_dirty',\n",
       " 'mark_non_differentiable',\n",
       " 'mark_shared_storage',\n",
       " 'materialize_grads',\n",
       " 'maybe_clear_saved_tensors',\n",
       " 'metadata',\n",
       " 'name',\n",
       " 'needs_input_grad',\n",
       " 'next_functions',\n",
       " 'non_differentiable',\n",
       " 'register_hook',\n",
       " 'register_prehook',\n",
       " 'requires_grad',\n",
       " 'save_for_backward',\n",
       " 'save_for_forward',\n",
       " 'saved_for_forward',\n",
       " 'saved_tensors',\n",
       " 'saved_variables',\n",
       " 'set_materialize_grads',\n",
       " 'setup_context',\n",
       " 'to_save',\n",
       " 'vjp',\n",
       " 'vmap']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.autograd.Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can set `requires_grad=True` for the weights we want to calculate gradients for. usually we set this for leaf nodes(w, b)\n",
    "\n",
    "if x has `requires_grad=True` ==> y = f(x) also has `requires_grad=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<SigmoidBackward0 object at 0x000001BF36DD1D20>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True, dtype=torch.float64)\n",
    "y = torch.sigmoid(x)\n",
    "z = 2*y\n",
    "print(z.requires_grad)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1/(1+np.exp(-x.item())), 6) == round(y.item(), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float64)\n",
      "None\n",
      "None\n",
      "True\n",
      "<bound method Tensor.backward of tensor(1., dtype=torch.float64, requires_grad=True)>\n"
     ]
    }
   ],
   "source": [
    "print(x.data)\n",
    "print(x.grad)\n",
    "print(x.grad_fn)\n",
    "print(x.requires_grad)\n",
    "print(x.backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we make `requires_grad=False` in x then y also becomes a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False tensor(0.7311, dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf, y.is_leaf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1966, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19661193324148185"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.item()*(1-y.item()) #gradient of sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
